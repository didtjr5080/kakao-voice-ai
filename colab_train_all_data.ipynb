{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“± ì¹´ì¹´ì˜¤í†¡ + ì¸ìŠ¤íƒ€ê·¸ë¨ í†µí•© í•™ìŠµ\n",
    "\n",
    "## ìˆœì„œ\n",
    "1. í™˜ê²½ ì„¤ì •\n",
    "2. ë°ì´í„° ì—…ë¡œë“œ (CSV + TXT)\n",
    "3. ë°ì´í„° ì „ì²˜ë¦¬\n",
    "4. ëª¨ë¸ í•™ìŠµ\n",
    "5. ë‹¤ìš´ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 1. í™˜ê²½ ì„¤ì • =====\n",
    "print('ğŸ”§ Installing packages...')\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU í™•ì¸\n",
    "import torch\n",
    "print(f'GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"Not available\"}')\n",
    "print(f'CUDA: {torch.cuda.is_available()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 2. ë°ì´í„° íŒŒì„œ í´ë˜ìŠ¤ =====\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "\n",
    "class UnifiedChatParser:\n",
    "    \"\"\"ì¹´ì¹´ì˜¤í†¡ + ì¸ìŠ¤íƒ€ê·¸ë¨ í†µí•© íŒŒì„œ\"\"\"\n",
    "    \n",
    "    def __init__(self, your_name: str = \"ë‚˜\"):\n",
    "        self.your_name = your_name\n",
    "    \n",
    "    def parse_kakao_csv(self, csv_path: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"ì¹´ì¹´ì˜¤í†¡ CSV íŒŒì‹±\"\"\"\n",
    "        print(f\"\\nğŸ“± Parsing KakaoTalk: {csv_path}\")\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        pairs = []\n",
    "        \n",
    "        prev_msg = None\n",
    "        prev_sender = None\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            sender = row['User']\n",
    "            message = str(row['Message']).strip()\n",
    "            \n",
    "            if not message or message == 'nan' or len(message) < 1 or len(message) > 200:\n",
    "                continue\n",
    "            \n",
    "            message = self._clean(message)\n",
    "            \n",
    "            if prev_msg and prev_sender != sender:\n",
    "                pairs.append((prev_msg, message))\n",
    "            \n",
    "            prev_msg = message\n",
    "            prev_sender = sender\n",
    "        \n",
    "        print(f\"  âœ… {len(pairs)} pairs extracted\")\n",
    "        return pairs\n",
    "    \n",
    "    def parse_instagram_txt(self, txt_path: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"ì¸ìŠ¤íƒ€ê·¸ë¨ TXT íŒŒì‹±\"\"\"\n",
    "        print(f\"\\nğŸ“· Parsing Instagram: {txt_path}\")\n",
    "        \n",
    "        with open(txt_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        pairs = []\n",
    "        messages = []\n",
    "        \n",
    "        # íŒ¨í„´: [ì´ë¦„] ë‚ ì§œ\\n\\të©”ì‹œì§€\n",
    "        pattern = r'\\[([^\\]]+)\\]\\s+(\\d{4}ë…„[^\\n]+)\\n\\s+(.+?)(?=\\n\\[|\\Z)'\n",
    "        \n",
    "        for match in re.finditer(pattern, content, re.DOTALL):\n",
    "            sender = match.group(1).strip()\n",
    "            message = match.group(3).strip()\n",
    "            \n",
    "            # íƒ­, ì¤„ë°”ê¿ˆ ì •ë¦¬\n",
    "            message = ' '.join(message.split())\n",
    "            message = self._clean(message)\n",
    "            \n",
    "            if len(message) < 1 or len(message) > 200:\n",
    "                continue\n",
    "            \n",
    "            messages.append({'sender': sender, 'text': message})\n",
    "        \n",
    "        # ëŒ€í™” ìŒ ìƒì„±\n",
    "        for i in range(len(messages) - 1):\n",
    "            if messages[i]['sender'] != messages[i+1]['sender']:\n",
    "                pairs.append((messages[i]['text'], messages[i+1]['text']))\n",
    "        \n",
    "        print(f\"  âœ… {len(pairs)} pairs extracted\")\n",
    "        return pairs\n",
    "    \n",
    "    def _clean(self, text: str) -> str:\n",
    "        \"\"\"í…ìŠ¤íŠ¸ ì •ë¦¬\"\"\"\n",
    "        # URL ì œê±°\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        # ì´ë©”ì¼ ì œê±°\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        # ì´ë¦„ ì œê±° (ê°œì¸ì •ë³´)\n",
    "        text = re.sub(r'ì‚¼ê´´[ê°€-í£]+', '', text)\n",
    "        # ê³µë°± ì •ë¦¬\n",
    "        text = ' '.join(text.split())\n",
    "        return text.strip()\n",
    "    \n",
    "    def merge_and_deduplicate(self, all_pairs: List[List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"ë³‘í•© ë° ì¤‘ë³µ ì œê±°\"\"\"\n",
    "        print(\"\\nğŸ”„ Merging and deduplicating...\")\n",
    "        \n",
    "        merged = []\n",
    "        seen = set()\n",
    "        \n",
    "        for pairs in all_pairs:\n",
    "            for inp, out in pairs:\n",
    "                key = (inp.lower(), out.lower())\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    merged.append((inp, out))\n",
    "        \n",
    "        print(f\"  âœ… Total unique pairs: {len(merged)}\")\n",
    "        return merged\n",
    "\n",
    "print('âœ… Parser class loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 3. ë°ì´í„° ì—…ë¡œë“œ =====\n",
    "\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "print('ğŸ“¤ Upload ALL your chat files (CSV for KakaoTalk, TXT for Instagram):')\n",
    "print('   You can select multiple files at once!')\n",
    "uploaded = files.upload()\n",
    "\n",
    "# íŒŒì¼ ë¶„ë¥˜\n",
    "kakao_files = [f for f in uploaded.keys() if f.endswith('.csv')]\n",
    "instagram_files = [f for f in uploaded.keys() if f.endswith('.txt')]\n",
    "\n",
    "print(f\"\\nâœ… Uploaded:\")\n",
    "print(f\"   ğŸ“± KakaoTalk: {len(kakao_files)} files\")\n",
    "print(f\"   ğŸ“· Instagram: {len(instagram_files)} files\")\n",
    "\n",
    "# íŒŒì¼ ì´ë™\n",
    "for f in uploaded.keys():\n",
    "    !mv \"{f}\" data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 4. ë°ì´í„° ì „ì²˜ë¦¬ =====\n",
    "\n",
    "parser = UnifiedChatParser(your_name=\"ë‚˜\")  # ë³¸ì¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½\n",
    "\n",
    "all_pairs = []\n",
    "\n",
    "# ì¹´ì¹´ì˜¤í†¡ íŒŒì¼ íŒŒì‹±\n",
    "for csv_file in kakao_files:\n",
    "    try:\n",
    "        pairs = parser.parse_kakao_csv(f'data/{csv_file}')\n",
    "        all_pairs.append(pairs)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error: {e}\")\n",
    "\n",
    "# ì¸ìŠ¤íƒ€ê·¸ë¨ íŒŒì¼ íŒŒì‹±\n",
    "for txt_file in instagram_files:\n",
    "    try:\n",
    "        pairs = parser.parse_instagram_txt(f'data/{txt_file}')\n",
    "        all_pairs.append(pairs)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error: {e}\")\n",
    "\n",
    "# ë³‘í•© ë° ì¤‘ë³µ ì œê±°\n",
    "training_pairs = parser.merge_and_deduplicate(all_pairs)\n",
    "\n",
    "# ìƒ˜í”Œ ì¶œë ¥\n",
    "print(\"\\nğŸ“ Sample conversations:\")\n",
    "for i, (inp, out) in enumerate(training_pairs[:10], 1):\n",
    "    print(f\"{i:2}. User: {inp}\")\n",
    "    print(f\"    Bot:  {out}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 5. ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜ =====\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "\n",
    "class LightweightChatbot:\n",
    "    def __init__(self, model_name=\"skt/kogpt2-base-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "    \n",
    "    def train(self, training_pairs, output_dir=\"./kakao_instagram_model\", epochs=5, batch_size=4):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸš€ Starting Training\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì €\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        special_tokens = {'additional_special_tokens': ['<|user|>', '<|bot|>', '<|end|>']}\n",
    "        self.tokenizer.add_special_tokens(special_tokens)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # ëª¨ë¸ (4bit ì–‘ìí™”)\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        # LoRA ì„¤ì •\n",
    "        peft_config = LoraConfig(\n",
    "            r=16,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"c_attn\", \"c_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\"\n",
    "        )\n",
    "        \n",
    "        self.model = get_peft_model(self.model, peft_config)\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "        training_texts = [f\"<|user|>{inp}<|bot|>{out}<|end|>\" for inp, out in training_pairs]\n",
    "        dataset = Dataset.from_dict({'text': training_texts})\n",
    "        \n",
    "        def tokenize(examples):\n",
    "            return self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                max_length=256,\n",
    "                padding='max_length'\n",
    "            )\n",
    "        \n",
    "        tokenized = dataset.map(tokenize, batched=True, remove_columns=['text'])\n",
    "        \n",
    "        # í•™ìŠµ ì„¤ì •\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "            save_total_limit=2,\n",
    "            warmup_steps=100,\n",
    "            gradient_checkpointing=True,\n",
    "            optim=\"adamw_torch\",\n",
    "            report_to=\"none\"\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized,\n",
    "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n",
    "        )\n",
    "        \n",
    "        # í•™ìŠµ ì‹œì‘\n",
    "        print(f\"\\nğŸ“Š Training on {len(training_pairs)} conversation pairs\")\n",
    "        trainer.train()\n",
    "        \n",
    "        # ì €ì¥\n",
    "        print(f\"\\nğŸ’¾ Saving to {output_dir}\")\n",
    "        trainer.save_model(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        \n",
    "        print(\"\\nâœ… Training Complete!\")\n",
    "        return output_dir\n",
    "\n",
    "print('âœ… Model class loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 6. í•™ìŠµ ì‹¤í–‰ =====\n",
    "\n",
    "chatbot = LightweightChatbot()\n",
    "\n",
    "output_dir = chatbot.train(\n",
    "    training_pairs=training_pairs,\n",
    "    output_dir=\"./kakao_instagram_model\",\n",
    "    epochs=5,           # ë°ì´í„° ë§ìœ¼ë©´ 3-5ë¡œ\n",
    "    batch_size=4        # Colab GPUì— ë§ê²Œ ì¡°ì •\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 7. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ =====\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"ğŸ§ª Loading model for testing...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(base_model, output_dir)\n",
    "model.eval()\n",
    "\n",
    "def test_generate(user_input):\n",
    "    prompt = f\"<|user|>{user_input}<|bot|>\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=30,\n",
    "            temperature=0.7,\n",
    "            top_p=0.85,\n",
    "            repetition_penalty=1.8,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    if '<|bot|>' in response:\n",
    "        response = response.split('<|bot|>')[-1].split('<|end|>')[0].strip()\n",
    "    return response\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "test_inputs = [\"ì•ˆë…•\", \"ë­í•´?\", \"ã…‹ã…‹ã…‹\", \"ì˜¤ëŠ˜ ë­ ë¨¹ì„ê¹Œ?\", \"ê²Œì„í•˜ì\"]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ Test Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for inp in test_inputs:\n",
    "    response = test_generate(inp)\n",
    "    print(f\"User: {inp}\")\n",
    "    print(f\"Bot:  {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== 8. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ =====\n",
    "\n",
    "print(\"ğŸ“¦ Zipping model...\")\n",
    "!zip -r kakao_instagram_model.zip {output_dir}\n",
    "\n",
    "print(\"\\nâ¬‡ï¸ Downloading...\")\n",
    "files.download('kakao_instagram_model.zip')\n",
    "\n",
    "print(\"\\nâœ… Complete! Extract and use with your local app.py\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
