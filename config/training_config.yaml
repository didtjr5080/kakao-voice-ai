training:
  epochs: 3
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  warmup_steps: 50
  max_length: 256
  
  optimization:
    use_fp16: true
    gradient_checkpointing: true
    optim: "adamw_torch"
  
  logging:
    steps: 10
  
  saving:
    steps: 100
    total_limit: 2

data:
  context_window: 1
  train_test_split: 0.9
